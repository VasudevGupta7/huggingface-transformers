{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3613jvsc74a57bd0d473bd2969313341ae55107d147e3bbb806c861b469038fcda3e98289e5dd186",
   "display_name": "Python 3.6.13 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "d473bd2969313341ae55107d147e3bbb806c861b469038fcda3e98289e5dd186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigbird.core import modeling\n",
    "import tensorflow.compat.v2 as tf\n",
    "from tqdm import tqdm\n",
    "from transformers import BigBirdPegasusConfig, BigBirdPegasusModel, BigBirdPegasusForConditionalGeneration, BigBirdPegasusTokenizer, BigBirdConfig, EncoderDecoderConfig, EncoderDecoderModel\n",
    "from bigbird.summarization.run_summarization import serving_input_fn_builder\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "tf.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = BigBirdPegasusTokenizer(\"tf_ckpt/spiece.model\")\n",
    "# t.save_pretrained(\"google/bigbird-pegasus-large-arxiv\")\n",
    "# t = BigBirdPegasusTokenizer.from_pretrained(\"google/bigbird-pegasus-large-arxiv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# o = t(\"This is a long example input string containing special characters .\\n$?-, numbers 2872 234 12 and words.\", max_length=30, padding=\"max_length\").input_ids\n",
    "\n",
    "# print(o)\n",
    "# # t.convert_ids_to_tokens(o)\n",
    "# ifn = serving_input_fn_builder(batch_size=1, max_encoder_length=30, vocab_model_file=\"tf_ckpt/spiece.model\", substitute_newline=False)\n",
    "\n",
    "# ifn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference_between_tensors(tf_tensor, pt_tensor):\n",
    "    tf_np = np.array(tf_tensor)\n",
    "    pt_np = np.array(pt_tensor.detach())\n",
    "    return np.max(np.abs(tf_np - pt_np))\n",
    "\n",
    "TF_CKPT_DIR = \"tf_ckpt/bigbird-roberta-arxiv/model.ckpt-300000\"\n",
    "HF_CKPT_DIR = \"google/bigbird-roberta-arxiv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'couple_encoder_decoder' is switching pegasus & encoder-decoder\n",
    "\n",
    "bbc = {\n",
    "      # transformer basic configs\n",
    "      \"couple_encoder_decoder\": True,\n",
    "      \"vocab_size\": 50358,\n",
    "      \"attention_probs_dropout_prob\": 0.0,\n",
    "      \"hidden_act\": \"gelu\",\n",
    "      \"hidden_dropout_prob\": 0.0,\n",
    "      \"hidden_size\": 768,\n",
    "      \"initializer_range\": 0.02,\n",
    "      \"intermediate_size\": 3072,\n",
    "      \"max_position_embeddings\": 4096,\n",
    "      \"num_attention_heads\": 12,\n",
    "      \"num_hidden_layers\": 12,\n",
    "      \"num_decoder_layer\": 12,\n",
    "      \"type_vocab_size\": 1,\n",
    "      \"use_bias\": True,\n",
    "      \"rescale_embedding\": False,\n",
    "      \"scope\": \"bert\",\n",
    "      # sparse mask configs\n",
    "      \"attention_type\": \"original_full\", # \"block_sparse\" \"original_full\"\n",
    "      \"norm_type\": \"postnorm\",\n",
    "      \"block_size\": 16,\n",
    "      \"num_rand_blocks\": 3,\n",
    "      # common bert configs\n",
    "      \"max_encoder_length\": 256,\n",
    "      \"max_decoder_length\": 16,\n",
    "      \"batch_size\": 1,\n",
    "      \"beam_size\": 5, #\n",
    "      \"alpha\": 0.1, #\n",
    "}\n",
    "# hf_bigbird_config = BigBirdConfig(\n",
    "#         vocab_size=bbc['vocab_size'],\n",
    "#         hidden_size=768,\n",
    "#         num_hidden_layers=bbc[\"num_hidden_layers\"],\n",
    "#         num_attention_heads=bbc['num_attention_heads'],\n",
    "#         intermediate_size=bbc[\"intermediate_size\"],\n",
    "#         hidden_act=\"gelu_fast\",\n",
    "#         hidden_dropout_prob=0.1,\n",
    "#         attention_probs_dropout_prob=0.1,\n",
    "#         max_position_embeddings=4096,\n",
    "#         type_vocab_size=bbc['type_vocab_size'],\n",
    "#         initializer_range=0.02,\n",
    "#         layer_norm_eps=1e-12,\n",
    "#         use_cache=True,\n",
    "#         rescale_embeddings=False,\n",
    "#         attention_type=bbc['attention_type'], # only for encoder\n",
    "#         block_size=bbc['block_size'],\n",
    "#         num_random_blocks=bbc['num_rand_blocks'],\n",
    "#         use_bias=bbc['use_bias'],\n",
    "# )\n",
    "\n",
    "bigbird_config = bbc\n",
    "\n",
    "# b = BigBirdConfig.from_pretrained(\"google/bigbird-roberta-base\")\n",
    "# hf_config = EncoderDecoderConfig.from_encoder_decoder_configs(b, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = BigBirdConfig.from_pretrained(\"google/bigbird-roberta-base\", block_size=bbc['block_size'], num_random_blocks=bbc['num_rand_blocks'])\n",
    "hf_config = EncoderDecoderConfig.from_pretrained(\"google/bigbird-roberta-arxiv\", encoder=c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "BigBirdConfig {\n",
       "  \"architectures\": [\n",
       "    \"BigBirdForPreTraining\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"attention_type\": \"block_sparse\",\n",
       "  \"block_size\": 16,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu_fast\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 4096,\n",
       "  \"model_type\": \"big_bird\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"num_random_blocks\": 3,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"rescale_embeddings\": false,\n",
       "  \"sep_token_id\": 66,\n",
       "  \"transformers_version\": \"4.5.0.dev0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_bias\": true,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50358\n",
       "}"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "hf_config.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = bigbird_config[\"batch_size\"]\n",
    "s2 = bigbird_config[\"max_encoder_length\"]\n",
    "s3 = bigbird_config[\"max_decoder_length\"]\n",
    "\n",
    "np.random.seed(0)\n",
    "arr = np.random.randint(1, s2, size=s1*s2).reshape(s1, s2)\n",
    "input_ids = tf.convert_to_tensor(arr, dtype=tf.int32)\n",
    "hf_input_ids = torch.from_numpy(arr).long()\n",
    "\n",
    "np.random.seed(0)\n",
    "arr = np.random.randint(1, s3, size=s1*s3).reshape(s1, s3)\n",
    "target_ids = tf.convert_to_tensor(arr, dtype=tf.int32)\n",
    "hf_target_ids = torch.from_numpy(arr).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model = EncoderDecoderModel(config=hf_config)\n",
    "hf_model.load_state_dict(torch.load(os.path.join(HF_CKPT_DIR, \"pytorch_model.bin\")))\n",
    "hf_model.eval()\n",
    "for p in hf_model.parameters():\n",
    "    p.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'block_sparse'"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "hf_model.encoder.config.attention_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "100%|██████████| 316/316 [00:01<00:00, 246.42it/s]\n"
     ]
    }
   ],
   "source": [
    "model = modeling.TransformerModel(bigbird_config)\n",
    "o = model(input_ids, target_ids=target_ids)\n",
    "del o\n",
    "\n",
    "ckpt_reader = tf.compat.v1.train.NewCheckpointReader(TF_CKPT_DIR)\n",
    "model.set_weights([ckpt_reader.get_tensor(v.name[:-2]) for v in tqdm(model.trainable_weights, position=0)])\n",
    "\n",
    "model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n",
      "INFO:absl:**** Using original full attention ****\n"
     ]
    }
   ],
   "source": [
    "tf_out = model(input_ids, target_ids=target_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_out = hf_model(input_ids=hf_input_ids, decoder_input_ids=hf_target_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 16, 50358), dtype=float32, numpy=\n",
       "array([[[ 2.7104834e-01, -5.6082445e-01, -2.1321970e-01, ...,\n",
       "         -4.0912282e-01, -1.3124352e+00, -1.2683020e+00],\n",
       "        [-2.2986460e-01, -9.4347262e-01,  1.3863336e+00, ...,\n",
       "          1.8453598e-04, -1.3332489e+00, -1.4688762e+00],\n",
       "        [ 4.4439459e-01, -1.1039134e+00,  2.9542822e-01, ...,\n",
       "         -1.5371733e+00, -9.6540868e-01, -8.9439464e-01],\n",
       "        ...,\n",
       "        [ 9.7916800e-01, -6.8775547e-01,  1.3556516e-01, ...,\n",
       "         -1.4633611e+00, -9.5423007e-01, -9.6981692e-01],\n",
       "        [ 1.0442281e+00, -4.0241051e-01,  1.5293916e-01, ...,\n",
       "         -1.3055569e+00, -8.4877443e-01, -8.5212266e-01],\n",
       "        [ 1.0604000e+00, -1.4837348e-01,  1.7223862e-01, ...,\n",
       "         -1.3593765e+00, -8.5776043e-01, -8.7298346e-01]]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "tf_out[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[ 6.6704e-01,  8.3960e-01,  1.5101e+00,  ..., -1.3282e+00,\n",
       "           2.5375e-01, -9.3701e-03],\n",
       "         [ 6.6382e-01,  8.3080e-01,  1.5285e+00,  ..., -1.2450e+00,\n",
       "           2.7344e-01,  4.9897e-04],\n",
       "         [ 5.9984e-01,  8.0295e-01,  1.4645e+00,  ..., -1.2910e+00,\n",
       "           2.4942e-01, -1.5131e-02],\n",
       "         ...,\n",
       "         [ 5.9278e-01,  8.0517e-01,  1.4685e+00,  ..., -1.2839e+00,\n",
       "           2.5876e-01, -1.0901e-02],\n",
       "         [ 5.9458e-01,  8.0307e-01,  1.4751e+00,  ..., -1.2913e+00,\n",
       "           2.5816e-01, -1.1425e-02],\n",
       "         [ 5.9999e-01,  8.1174e-01,  1.4873e+00,  ..., -1.2903e+00,\n",
       "           2.6024e-01, -9.9632e-03]]])"
      ]
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "source": [
    "hf_out['logits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = set([v.name[:-2] for v in model.trainable_variables])\n",
    "# b = set([b[0] for b in tf.train.list_variables(TF_CKPT_DIR)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "difference in encoder out 0.0009796619\ndifference in encoder out 0.0009796619\ndifference in final out 0.0019226074\n"
     ]
    }
   ],
   "source": [
    "print(\"difference in encoder out\", difference_between_tensors(model.encoder_o, hf_model.model.encoder.encoder_o))\n",
    "\n",
    "print(\"difference in encoder out\", difference_between_tensors(tf_out[1], hf_out['encoder_last_hidden_state']))\n",
    "\n",
    "print(\"difference in final out\", difference_between_tensors(tf_out[0][1], hf_out['logits']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 3.7736,  0.6459,  5.9393, -2.0550,  1.3957,  1.6994,  1.7002,  4.3194,\n",
       "          6.7270,  0.8877,  2.7457,  0.3128, -0.3091,  3.7636,  6.4191,  3.2155,\n",
       "         -0.9953,  7.4407,  3.8938,  0.4070,  3.7436,  3.7248,  5.6073,  3.8378,\n",
       "         -1.9400,  5.2315,  4.6829,  2.0397],\n",
       "        [ 3.8075,  0.5993,  5.9881, -1.9268,  1.7395,  1.9801,  1.4785,  4.4040,\n",
       "          6.9427,  0.6825,  2.8742,  0.7088, -0.6241,  3.3309,  6.5836,  3.2848,\n",
       "         -1.1375,  7.2144,  4.1101,  0.8657,  3.9520,  3.5079,  5.4696,  3.9301,\n",
       "         -2.2243,  5.2562,  4.6510,  1.9688],\n",
       "        [ 3.9393,  0.5811,  6.1118, -1.9829,  1.9584,  2.0622,  1.6118,  4.5815,\n",
       "          7.1832,  0.6703,  2.9474,  0.8766, -0.7241,  3.3090,  6.7720,  3.4544,\n",
       "         -1.0948,  7.0197,  4.2286,  1.1543,  4.0334,  3.4939,  5.5613,  4.1545,\n",
       "         -2.2169,  5.4238,  4.7881,  1.9614],\n",
       "        [ 4.0004,  0.5768,  6.1671, -2.1092,  2.0556,  2.0222,  1.5487,  4.5812,\n",
       "          7.2975,  0.7099,  3.0134,  0.9069, -0.8406,  3.2854,  6.7560,  3.5334,\n",
       "         -1.2231,  6.8766,  4.3854,  1.1062,  3.9816,  3.6632,  5.6403,  4.3885,\n",
       "         -2.2414,  5.4234,  4.7948,  1.9947]])"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "hf_out.logits[0, 4:8, 128:156]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"difference in embed out\", difference_between_tensors(model.embed_o, hf_model.model.encoder.embed_o))\n",
    "\n",
    "# print(\"difference in before_attn_o out\", difference_between_tensors(model.encoder.encoder_layers[0].before_attn_o, hf_model.model.encoder.layers[0].before_attn_o))\n",
    "\n",
    "# print(\"difference in after self_o out\", difference_between_tensors(tf.reshape(model.encoder.encoder_layers[0].self_o, (1, 128, 1024)), hf_model.model.encoder.layers[0].self_attn.self_o))\n",
    "\n",
    "# print(\"difference in after so_o out\", difference_between_tensors(model.encoder.encoder_layers[0].so_o, hf_model.model.encoder.layers[0].self_attn.so_o))\n",
    "\n",
    "# print(\"difference in after self-attn out\", difference_between_tensors(model.encoder.encoder_layers[0].after_attn_o, hf_model.model.encoder.layers[0].after_attn_o))\n",
    "\n",
    "# print(\"difference in before inter out\", difference_between_tensors(model.encoder.encoder_layers[0].before_inter_o, hf_model.model.encoder.layers[0].before_inter_o))\n",
    "\n",
    "# print(\"difference in after inter out\", difference_between_tensors(model.encoder.encoder_layers[0].after_inter_o, hf_model.model.encoder.layers[0].after_inter_o))\n",
    "\n",
    "# print(\"difference in output out\", difference_between_tensors(model.encoder.encoder_layers[0].output_o, hf_model.model.encoder.layers[0].output_o))\n",
    "\n",
    "# print(\"difference in l0 out\", difference_between_tensors(model.encoder.l0_o, hf_model.model.encoder.l0_o))\n",
    "\n",
    "# print(\"difference in l last out\", difference_between_tensors(model.encoder.llast_o, hf_model.model.encoder.llast_o))\n",
    "\n",
    "# print(\"difference in ki\", difference_between_tensors(model.encoder.encoder_layers[0].attn_layer.ki, hf_model.model.encoder.layers[0].self_attn.self.qi))\n",
    "# print(\"difference in qi\", difference_between_tensors(model.encoder.encoder_layers[0].attn_layer.qi, hf_model.model.encoder.layers[0].self_attn.self.qi))\n",
    "\n",
    "# print(\"difference in qo out\", difference_between_tensors(model.encoder.encoder_layers[0].attn_layer.qo, hf_model.model.encoder.layers[0].self_attn.self.qo))\n",
    "\n",
    "# print(\"difference in ko out\", difference_between_tensors(model.encoder.encoder_layers[0].attn_layer.ko, hf_model.model.encoder.layers[0].self_attn.self.ko))\n",
    "\n",
    "# print(\"difference in vo out\", difference_between_tensors(model.encoder.encoder_layers[0].attn_layer.vo, hf_model.model.encoder.layers[0].self_attn.self.vo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigbird pegasus large pubmed\n",
    "# difference in encoder out 0.0002682209\n",
    "# difference in encoder out 0.0002682209\n",
    "# difference in final out 0.0008444786\n",
    "\n",
    "# bigbird pegasus large bigpatent\n",
    "# difference in encoder out 0.00029605627\n",
    "# difference in encoder out 0.00029605627\n",
    "# difference in final out 0.00074386597\n",
    "\n",
    "# bigbird pegasus large arxiv\n",
    "# difference in encoder out 0.0005502105\n",
    "# difference in encoder out 0.0005502105\n",
    "# difference in final out 0.00051164627\n",
    "\n",
    "\n",
    "# bigbird pegasus large\n",
    "# difference in encoder out 0.00011986494\n",
    "# difference in encoder out 0.00011986494\n",
    "# difference in final out 0.012252808"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EncoderDecoderModel, BigBirdModel, EncoderDecoderConfig, BigBirdConfig\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = EncoderDecoderModel.from_encoder_decoder_pretrained(\"google/bigbird-roberta-base\", \"google/bigbird-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [k for k in model.state_dict().keys()]\n",
    "\n",
    "# token_type_embeddings = None\n",
    "# position_ids = torch.arange(config.max_position_embeddings).expand((1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [(c[0], c[1]) for c in tf.train.list_variables(\"tf_ckpt/bigbird-roberta-arxiv/model.ckpt-300000\")]\n",
    "# encoder.encoder.layer.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [k for k in model.state_dict().keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "When using `BigBirdForCausalLM` as decoder, then `attention_type` must be `original_full`. Setting `attention_type=original_full`\n",
      "When using `BigBirdForCausalLM` as decoder, then `attention_type` must be `original_full`. Setting `attention_type=original_full`\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_out = hf_model(input_ids=hf_input_ids, labels=hf_target_ids)"
   ]
  }
 ]
}